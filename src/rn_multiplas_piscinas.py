# -*- coding: utf-8 -*-
"""RN- multiplas piscinas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q6j5fMxvUE6xOXIyONTncToY8Re_qpo0

# 1. Dependências

Foram usadas as bibliotecas:
- Pandas
- Bokeh
- Keras + Tensorflow
- Scikit-learn

- Numpy
- Datetime
- OS
"""

# Commented out IPython magic to ensure Python compatibility.
# @title Instalando dependências
# install TensorFlow 2.0 alpha version
# !pip install tensorflow==2.0.0-alpha0


# Load the TensorBoard notebook extension
# %load_ext tensorboard.notebook

# multivariate one step problem with lstm
import tensorflow as tf
import os
import datetime
import matplotlib.pyplot as plt
import seaborn as sns


from numpy import array
from numpy import hstack
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator

# Validação
from sklearn.pipeline import Pipeline
from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import cross_val_score
# from keras.wrappers.scikit_learn import KerasRegressor

from sklearn.preprocessing import OneHotEncoder
import numpy as np

import pandas as pd

# %load_ext google.colab.data_table

"""# 2. Base de dados

## 2.1. Define a base de dados

- Apresentar e fazer uma breve analise da base de dados
"""

# pool_name = "Adelphi"
# pool_name = "McCarren"

df_poolsize = pd.read_csv(
    "https://raw.githubusercontent.com/lmlima/PoolAttendance/master/data/pool_size.csv?token=ABOSJ766O57N7ELM3W25RG2566CKU")

df = pd.read_csv(
    "https://raw.githubusercontent.com/lmlima/PoolAttendance/master/data/ospa-completo.csv?token=ABOSJ75UBQYNWR44KEAYWPK566CL4")
df.head()
pools = df["Pool"].unique()

df_orig = df.copy()
# df_few_orig = df_orig[df_orig.Pool == pool_name].drop(columns="Pool")
# df_dummy = pd.get_dummies(df_orig["Pool"])

# input_col = ["Attendance", "Daily Data Temperaturemin", "Daily Data Temperaturemax"]
input_col = ["Pool", "Attendance", "Daily Data Temperaturehigh", "Daily Data Temperaturemax",
             "Daily Data Apparenttemperaturehigh", "Daily Data Apparenttemperaturemax", "Daily Data Temperaturelow",
             "Daily Data Apparenttemperaturelow", "Daily Data Uvindex", "Daily Data Apparenttemperaturemin",
             "Daily Data Temperaturemin", "Daily Data Windbearing", "Daily Data Dewpoint", "Daily Data Moonphase"]
df_few = df_orig[input_col]

"""Prepara o dicionario de sequência de dados das piscinas"""


# Separar a base de dades em treino, validacao e teste
def split_train_val_test(x, y, proporcao=0.8):
    tam = y.shape[0]

    tam_train = int(tam * proporcao)
    # val e test de mesmo tamanho
    tam_val = int((tam - tam_train) / 2)
    tam_test = int((tam - tam_train) / 2)

    x_train = x[:tam_train, :]
    x_val = x[tam_train:tam_train + tam_val, :]
    x_test = x[tam_train + tam_val:tam_train + tam_val + tam_test, :]

    y_train = y[:tam_train, :]
    y_val = y[tam_train:tam_train + tam_val, :]
    y_test = y[tam_train + tam_val:tam_train + tam_val + tam_test, :]

    return {"train": (x_train, y_train), "val": (x_val, y_val), "test": (x_test, y_test)}




# Proporcao train/val/test = 80/10/10
proporcao = 0.6

enc = OneHotEncoder(sparse=False, dtype=np.int)
enc.fit(df["Pool"].to_numpy().reshape(-1, 1))

dict_dataset = {}

for pool in pools:
    df_pool = df_few[df_orig.Pool == pool].drop(columns="Pool")

    # Adicionar a previsao do dia seguinte
    df_pool["Daily Data Apparenttemperaturemax -1"] = df_pool["Daily Data Apparenttemperaturemax"].shift(-1)
    df_pool["Daily Data Apparenttemperaturemin  -1"] = df_pool["Daily Data Apparenttemperaturemin"].shift(-1)

    df_pool = df_pool.reset_index(drop=True)

    a = enc.transform(array([[pool] for i in range(df_pool.shape[0])]))
    b = df_pool.to_numpy()
    x = np.concatenate((a, b), axis=1)

    y = df_pool[["Attendance"]].to_numpy()

    # split train/val/test
    splitted_data = split_train_val_test(x, y, proporcao=proporcao)

    # dict_dataset[pool] =  (x, y)
    dict_dataset[pool] = splitted_data

# dict_dataset['Asser Levy']["train"].shape

# print(F"Tamanho: {tam}")
# print(F"Train/val/test = {tam_train}/{tam_val}/{tam_test}")
# print(F"Total: {tam_train+tam_val+tam_test}")


# print(teste)
print(F"Tam train: {dict_dataset['Asser Levy']['train'][0].shape}")
print(F"Tam val: {dict_dataset['Asser Levy']['val'][0].shape}")
print(F"Tam test: {dict_dataset['Asser Levy']['test'][0].shape}")

"""## Modelo"""

window_size = 7
n_features = dict_dataset[pool]['train'][0].shape[1]
n_output = dict_dataset[pool]['train'][1].shape[1]


# define model
def lstm_model():
    # Arquitetura
    model = Sequential()
    model.add(LSTM(128, activation='relu', input_shape=(window_size, n_features)))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(n_output))

    # Otimizador
    # lr = 2e-6
    lr = 2e-5
    adam_opt = tf.keras.optimizers.Adam(learning_rate=lr)

    # Métricas e definição do modelo
    model.compile(optimizer=adam_opt, loss='mae', metrics=['mae', 'mse'])

    return model


model = lstm_model()
model.summary()

# Configuração do Tensorboard
logdir = os.path.join("logs/multi", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback_multi = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

"""## 3.2. Treinamento
- Apresentar os hiperparâmetros e pq
- Visualizar treinamento
"""
def data_generator(x, y, window_size):
    data_length = len(y)

    # Usar o TSGenerator para moldar os dados
    generator = TimeseriesGenerator(x, y, length=window_size, batch_size=data_length)
    X = generator[0][0]
    Y = generator[0][1]

    return (X, Y)


# Commented out IPython magic to ensure Python compatibility.
# @title Visualizar treinamento
# %tensorboard --logdir logs

"""
    Abordagem com um único modelo para todas as piscinas
"""
# fit model
# n_epochs = 1500
n_epochs = 50

n_batch = 256
# validation_perc = 0.3
model_Multi = lstm_model()

for i, pool in enumerate(pools):
    print(F"Treinamento piscina {i + 1}/{len(pools)}")

    dataset_train, target_train = dict_dataset[pool]["train"]
    dataset_val, target_val = dict_dataset[pool]["val"]

    X_train, Y_train = data_generator(dataset_train, target_train, window_size=window_size)
    X_val, Y_val = data_generator(dataset_val, target_val, window_size=window_size)

    model_Multi.fit(X_train, Y_train, batch_size=n_batch, validation_data=(X_val, Y_val), shuffle=False, epochs=n_epochs,
              callbacks=[tensorboard_callback_multi], verbose=0)

# Evaluate
lst = []
for i, pool in enumerate(pools):
    print(F"Avaliação piscina {i + 1}/{len(pools)}")

    dataset_test, target_test = dict_dataset[pool]["test"]
    X_test, Y_test = data_generator(dataset_test, target_test, window_size=window_size)

    model_eval = model_Multi.evaluate(X_test, Y_test, batch_size=n_batch)
    lst.append(model_eval)

eval_Multipool_hist = pd.DataFrame(lst, columns=model.metrics_names).drop(columns="loss")
eval_Multipool_hist["metodo"] = "Multi"

print("Abordagem Multi")
print(F"Epochs={n_epochs}\nBatch={n_batch}\n")
print(eval_Multipool_hist.describe())

"""
    Abordagem com um modelo para cada piscina
"""
# fit model
# n_epochs = 1500
n_epochs = 200

n_batch = 32
# validation_perc = 0.3
model_Single = {}


# Configuração do Tensorboard
logdir = os.path.join("logs/single", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback_single = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

for i, pool in enumerate(pools):
    print(F"Treinamento piscina {i + 1}/{len(pools)}")

    # Create model
    model_Single[pool] = lstm_model()

    dataset_train, target_train = dict_dataset[pool]["train"]
    dataset_val, target_val = dict_dataset[pool]["val"]

    X_train, Y_train = data_generator(dataset_train, target_train, window_size=window_size)
    X_val, Y_val = data_generator(dataset_val, target_val, window_size=window_size)

    model_Single[pool].fit(X_train, Y_train, batch_size=n_batch, validation_data=(X_val, Y_val), shuffle=False, epochs=n_epochs,
              callbacks=[tensorboard_callback_single], verbose=0)

# Evaluate
lst = []
for i, pool in enumerate(pools):
    print(F"Avaliação piscina {i + 1}/{len(pools)}")

    dataset_test, target_test = dict_dataset[pool]["test"]
    X_test, Y_test = data_generator(dataset_test, target_test, window_size=window_size)

    model_eval = model_Single[pool].evaluate(X_test, Y_test, batch_size=n_batch)
    lst.append(model_eval)

eval_Singlepool_hist = pd.DataFrame(lst, columns=model.metrics_names).drop(columns="loss")
eval_Singlepool_hist["metodo"] = "Single"


print("Abordagem Single")
print(F"Epochs={n_epochs}\nBatch={n_batch}\n")
print(eval_Singlepool_hist.describe())

"""
    Abordagem Naive
"""
# Evaluate Naive
lst = []
for i, pool in enumerate(pools):
    print(F"Avaliação piscina {i + 1}/{len(pools)}")

    dataset_test, target_test = dict_dataset[pool]["test"]
    # X_test, Y_test = data_generator(dataset_test, target_test, window_size=window_size)

    # Prediction
    df_target = pd.DataFrame(target_test, columns=["Attendance"])
    pred_naive = df_target["Attendance"].shift(1)
    # Ultimo do validacao é o primeiro predito do naive
    pred_naive[0] = dict_dataset[pool]["val"][1][-1]

    target_naive = target_test.reshape(1, -1)[0]

    # Calcula metricas
    mse = tf.keras.metrics.mean_squared_error(pred_naive.to_numpy(), target_naive).numpy()
    mae = tf.keras.metrics.mean_absolute_error(pred_naive.to_numpy(), target_naive).numpy()

    loss = 0

    model_eval = list((loss, mae, mse))
    # model_eval = model.evaluate(X_test, Y_test, batch_size=n_batch)
    lst.append(model_eval)

eval_Naive_hist = pd.DataFrame(lst, columns=model.metrics_names).drop(columns="loss")
eval_Naive_hist["metodo"] = "naive"

eval_Naive_hist.describe()

# Comparação com naive
print("Comparação desempenho por piscina")
eval_diff = eval_Multipool_hist.drop(columns="metodo") - eval_Naive_hist.drop(columns="metodo")
print("mse")
print(np.sign(eval_diff)["mse"].value_counts().rename(index={1.0: "multi", -1.0: "naive"}))
print("mae")
print(np.sign(eval_diff)["mae"].value_counts().rename(index={1.0: "multi", -1.0: "naive"}))

eval_diff = eval_Singlepool_hist.drop(columns="metodo") - eval_Naive_hist.drop(columns="metodo")
print("mse")
print(np.sign(eval_diff)["mse"].value_counts().rename(index={1.0: "single", -1.0: "naive"}))
print("mae")
print(np.sign(eval_diff)["mae"].value_counts().rename(index={1.0: "single", -1.0: "naive"}))

eval_diff = eval_Singlepool_hist.drop(columns="metodo") - eval_Multipool_hist.drop(columns="metodo")
print("mse")
print(np.sign(eval_diff)["mse"].value_counts().rename(index={1.0: "single", -1.0: "multi"}))
print("mae")
print(np.sign(eval_diff)["mae"].value_counts().rename(index={1.0: "single", -1.0: "multi"}))


evals = pd.concat([eval_Naive_hist.reset_index(),
           eval_Singlepool_hist.reset_index(),
           eval_Multipool_hist.reset_index()], axis="rows", ignore_index=True)

df_evals = pd.melt(evals, id_vars=['index', 'metodo'], var_name='metrica')
df_evals.to_pickle("logs/df_evals.pkl")
# col_mse = [k for k in evals.columns if 'mse' in k]
# col_mae = [k for k in evals.columns if 'mae' in k]

# Comparação total de erro
ax = sns.catplot(y="metodo", x="value", data=df_evals[df_evals.metrica=="mse"].drop(columns="metrica"), orient="h", kind="bar")
plt.title('MSE médio das piscinas')
plt.show()

sns.catplot(y="metodo", x="value", data=df_evals[df_evals.metrica=="mae"].drop(columns="metrica"), orient="h", kind="bar")
plt.title('MAE médio das piscinas')
plt.show()

# Comapracao por piscina
sns.catplot(y="index", x="value", hue="metodo", data=df_evals[df_evals.metrica=="mse"].drop(columns="metrica"), orient="h", kind="bar", height=18.27)
plt.title('MSE das piscinas')
plt.show()

sns.catplot(y="index", x="value", hue="metodo", data=df_evals[df_evals.metrica=="mae"].drop(columns="metrica"), orient="h", kind="bar", height=18.27)
plt.title('MAE das piscinas')
plt.show()


# """# 4. Cálculo de salva vidas
# Calcula o número de salva vidas necessário
# """
#
# pred_x = X[96:97]
# y_real = int(Y[96:97])
#
# print(y_real)
# print(F"Entrada:\n{pred_x}")
#
# y_pred = int(model.predict(pred_x))
#
# print(F"Presença real: {y_real}\nPresença predita: {y_pred}\nDiferença: {y_real - y_pred}")

# import math
#
#
# def NumSalvaVidasSup(attendance, Areapool):
#     if Areapool < 3400:
#         salvaVidasSup = math.ceil(attendance / 75)
#     elif Areapool >= 3400:
#         salvaVidasSup = math.ceil(Areapool / 3400)
#         if 1.5 * (Areapool / 25) < attendance:
#             salvaVidas1 = salvaVidasSup + 1
#     if salvaVidasSup < 1:
#         salvaVidasSup = 1
#     return salvaVidasSup
#
#
# pool_size = df_poolsize[df_poolsize.Pool == pool_name]["Size"].values
#
# nsv_real = NumSalvaVidasSup(y_real, pool_size)
# nsv_pred = NumSalvaVidasSup(y_pred, pool_size)
#
# print(
#     F"Piscina {pool_name}\nSalva vidas Supervisores real : {nsv_real}\nSalva vidas Supervisores predito: {nsv_pred}\nDiferença: {nsv_real - nsv_pred}")
#
# import math
#
#
# def NumSalvaVidas(attendance, Areapool):
#     salvaVidas = math.ceil(attendance / 25) + math.ceil(Areapool / 3400)
#
#     if salvaVidas < 1:
#         salvaVidas = 1
#     return salvaVidas
#
#
# pool_size = df_poolsize[df_poolsize.Pool == pool_name]["Size"].values
#
# nsv_real = NumSalvaVidas(y_real, pool_size)
# nsv_pred = NumSalvaVidas(y_pred, pool_size)
#
# print(
#     F"Piscina {pool_name}\nSalva vidas real: {nsv_real}\nSalva vidas predito: {nsv_pred}\nDiferença: {nsv_real - nsv_pred}")
